{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.contextual.ai/v1/lmunit\"\n",
    "\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"content-type\": \"application/json\",\n",
    "    \"authorization\": \"Bearer key-CLRoggUEDxqJn3DHU6hPHk3R5f6KL98IEgDBpISri1Iwp8ptg\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set Model and File Locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = \"sonnet35\" # gpt4o llama70b gpt4omini mistrallarge sonnet35\n",
    "\n",
    "if model == \"gpt4o\":\n",
    "    mwoz_eval_json = \"results/judge-results-mwoz/20250130_140218-4o/gpt-4o_c-gpt-4o_j.json\"\n",
    "    tau_retail_eval_json = \"results/judge-results-tau/20250131_152422-tau-4o-retail/tau-gpt-4o_j.json\"\n",
    "    tau_air_eval_json = \"results/judge-results-tau/20250131_152503-tau-4o-airline/tau-gpt-4o_j.json\"\n",
    "    human_eval_scores_json = \"agreement_scores/gpt4o-qual-results/human_llm_comparison.json\"\n",
    "    indices = {\n",
    "        \"pmul1537\": (16, 26),\n",
    "        \"40\": (44, 50),\n",
    "        \"6\": (50, 58)\n",
    "    }\n",
    "elif model == \"llama70b\":\n",
    "    mwoz_eval_json = \"results/judge-results-mwoz/20250131_012449-llama70/meta-llama_Llama-3.3-70B-Instruct-Turbo_c-gpt-4o_j.json\"\n",
    "    tau_retail_eval_json = \"results/judge-results-tau/20250208_030407-tau-llama70b-retail/tau-gpt-4o_j.json\"\n",
    "    tau_air_eval_json = \"results/judge-results-tau/20250208_024344-tau-llama70b-airline/tau-gpt-4o_j.json\"\n",
    "    human_eval_scores_json = \"agreement_scores/llama70b-qual-results/human_llm_comparison.json\"\n",
    "    indices = {\n",
    "        \"pmul1537\": (16, 26),\n",
    "        \"40\": (40, 45),\n",
    "        \"6\": (45, 50)\n",
    "    }\n",
    "elif model == \"gpt4omini\":\n",
    "    mwoz_eval_json = \"results/judge-results-mwoz/20250130_140439-4omini/gpt-4o-mini_c-gpt-4o_j.json\"\n",
    "    tau_retail_eval_json = \"results/judge-results-tau/20250131_152338-tau-4o-mini-retail/tau-gpt-4o_j.json\"\n",
    "    tau_air_eval_json = \"results/judge-results-tau/20250131_152226-tau-4o-mini-airline/tau-gpt-4o_j.json\"\n",
    "    human_eval_scores_json = \"agreement_scores/gpt4omini-qual-results/human_llm_comparison.json\"\n",
    "    indices = {\n",
    "        \"pmul1537\": (16, 26),\n",
    "        \"40\": (45, 51),\n",
    "        \"6\": (51, 59)\n",
    "    }\n",
    "elif model == \"mistrallarge\":\n",
    "    mwoz_eval_json = \"results/judge-results-mwoz/20250130_184905-mistrallarge/mistral-large-latest_c-gpt-4o_j.json\"\n",
    "    tau_retail_eval_json = \"results/judge-results-tau/20250205_044403-tau-mistrallarge-retail/tau-gpt-4o_j.json\"\n",
    "    tau_air_eval_json = \"results/judge-results-tau/20250205_024823-tau-mistrallarge-airline/tau-gpt-4o_j.json\"\n",
    "    human_eval_scores_json = \"agreement_scores/mistrallarge-qual-results/human_llm_comparison.json\"\n",
    "    indices = {\n",
    "        \"pmul1537\": (16, 26),\n",
    "        \"40\": (46, 52),\n",
    "        \"6\": (52, 58)\n",
    "    }\n",
    "elif model == \"sonnet35\":\n",
    "    mwoz_eval_json = \"results/judge-results-mwoz/20250130_183030-claude/claude-3-5-sonnet-20241022_c-gpt-4o_j.json\"\n",
    "    tau_retail_eval_json = \"results/judge-results-tau/20250131_152807-tau-sonnet-retail/tau-gpt-4o_j.json\"\n",
    "    tau_air_eval_json = \"results/judge-results-tau/20250205_030422-tau-sonnet-airline/tau-gpt-4o_j.json\"\n",
    "    human_eval_scores_json = \"agreement_scores/sonnet35-qual-results/human_llm_comparison.json\"\n",
    "    indices = {\n",
    "        \"pmul1537\": (16, 26),\n",
    "        \"40\": (44, 51),\n",
    "        \"6\": (51, 56)\n",
    "    }\n",
    "else:\n",
    "    raise Error()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Dialogue Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(mwoz_eval_json, 'r') as f:\n",
    "    mwoz_eval = json.load(f)\n",
    "mwoz_dials = mwoz_eval.get('dialogues', [])\n",
    "\n",
    "with open(tau_retail_eval_json, 'r') as f:\n",
    "    tau_retail_eval = json.load(f)\n",
    "tau_retail_dials = tau_retail_eval['dialogues']\n",
    "\n",
    "with open(tau_air_eval_json, 'r') as f:\n",
    "    tau_air_eval = json.load(f)\n",
    "tau_air_dials = tau_air_eval['dialogues']\n",
    "# load batches\n",
    "dial_batches = \"datasets/short_dial_batch.json\"\n",
    "batch_list = None\n",
    "with open(dial_batches, 'r') as f:\n",
    "    batch_list = json.load(f)\n",
    "if batch_list is None or len(batch_list) == 0:\n",
    "    print('No batches found at this path:',  dial_batches)\n",
    "    exit()\n",
    "batch_order = batch_list[\"order\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter Dialogue Set For Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dials = {}\n",
    "mwoz_batch_ids = batch_list[\"mwoz\"]\n",
    "tau_retail_batch_ids = batch_list[\"tau\"][\"retail\"]\n",
    "tau_air_batch_ids = batch_list[\"tau\"][\"airline\"]\n",
    "for batch_id in mwoz_batch_ids:\n",
    "    for id, dial in mwoz_dials.items():\n",
    "        if id == batch_id:\n",
    "            batch_dials[id] = dial\n",
    "            break\n",
    "        \n",
    "for batch_id in tau_retail_batch_ids:\n",
    "    for id, dial in tau_retail_dials.items():\n",
    "        if id == batch_id:\n",
    "            batch_dials[id] = dial\n",
    "            break\n",
    "            \n",
    "for batch_id in tau_air_batch_ids:\n",
    "    for id, dial in tau_air_dials.items():\n",
    "        if id == batch_id:\n",
    "            batch_dials[id] = dial\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_dials = {}\n",
    "mwoz_batch_ids = batch_list[\"mwoz\"]\n",
    "tau_retail_batch_ids = batch_list[\"tau\"][\"retail\"]\n",
    "tau_air_batch_ids = batch_list[\"tau\"][\"airline\"]\n",
    "for batch_id in mwoz_batch_ids:\n",
    "    for id, dial in mwoz_dials.items():\n",
    "        if id == batch_id:\n",
    "            batch_dials[id] = dial\n",
    "            break\n",
    "        \n",
    "for batch_id in tau_retail_batch_ids:\n",
    "    for id, dial in tau_retail_dials.items():\n",
    "        if id == batch_id:\n",
    "            batch_dials[id] = dial\n",
    "            break\n",
    "            \n",
    "for batch_id in tau_air_batch_ids:\n",
    "    for id, dial in tau_air_dials.items():\n",
    "        if id == batch_id:\n",
    "            batch_dials[id] = dial\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate LMUnit Score on Dialogues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_qs = [\n",
    "    \"Does the response directly relate to the dialogue history and the current user query?\",\n",
    "    \"Does the response remain on-topic with the dialogue history and the user query?\",\n",
    "    \"Does the response logically continue the progression of the dialogue?\"\n",
    "]\n",
    "backend_qs = [\n",
    "    \"Does the response accurately reflect the information in the database results?\",\n",
    "    \"Does the response stay on-topic with the database results and the dialogue context?\",\n",
    "    \"Does response logically incorporate and progress based on the database results?\"\n",
    "]\n",
    "policy_qs = [\n",
    "    \"Does the response provide suggestions only when the database results are few enough to do so?\",\n",
    "    \"Does the response request required, relevant information from the user before offering suggestions or booking services?\",\n",
    "    \"Does the response avoid premature actions (i.e. make a booking or suggest a service) too early in the conversation, before the necessary information is gathered?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"pmul1537\": [{\"conv_consistency\": 3.76, \"backend_consistency\": 4.34, \"policy_completeness\": 4.34}, {\"conv_consistency\": 3.96, \"backend_consistency\": 3.97, \"policy_completeness\": 3.97}, {\"conv_consistency\": 2.59, \"backend_consistency\": 2.99, \"policy_completeness\": 2.99}, {\"conv_consistency\": 2.74, \"backend_consistency\": 2.84, \"policy_completeness\": 2.84}, {\"conv_consistency\": 3.75, \"backend_consistency\": 3.83, \"policy_completeness\": 3.83}, {\"conv_consistency\": 2.47, \"backend_consistency\": 2.31, \"policy_completeness\": 2.31}, {\"conv_consistency\": 3.88, \"backend_consistency\": 3.83, \"policy_completeness\": 3.83}, {\"conv_consistency\": 4.35, \"backend_consistency\": 4.29, \"policy_completeness\": 4.29}, {\"conv_consistency\": 3.43, \"backend_consistency\": 3.27, \"policy_completeness\": 3.27}, {\"conv_consistency\": 2.82, \"backend_consistency\": 2.38, \"policy_completeness\": 2.38}], \"40\": [{\"conv_consistency\": 3.74, \"backend_consistency\": 3.83, \"policy_completeness\": 3.83}, {\"conv_consistency\": 4.43, \"backend_consistency\": 4.37, \"policy_completeness\": 4.37}, {\"conv_consistency\": 4.6, \"backend_consistency\": 4.55, \"policy_completeness\": 4.55}, {\"conv_consistency\": 4.48, \"backend_consistency\": 4.5, \"policy_completeness\": 4.5}, {\"conv_consistency\": 4.21, \"backend_consistency\": 4.01, \"policy_completeness\": 4.01}, {\"conv_consistency\": 4.83, \"backend_consistency\": 4.8, \"policy_completeness\": 4.8}, {\"conv_consistency\": 4.86, \"backend_consistency\": 4.86, \"policy_completeness\": 4.86}], \"6\": [{\"conv_consistency\": 4.68, \"backend_consistency\": 4.63, \"policy_completeness\": 4.63}, {\"conv_consistency\": 4.04, \"backend_consistency\": 4.17, \"policy_completeness\": 4.17}, {\"conv_consistency\": 4.63, \"backend_consistency\": 4.65, \"policy_completeness\": 4.65}, {\"conv_consistency\": 4.78, \"backend_consistency\": 4.8, \"policy_completeness\": 4.8}, {\"conv_consistency\": 4.25, \"backend_consistency\": 4.18, \"policy_completeness\": 4.18}]}\n"
     ]
    }
   ],
   "source": [
    "from time import sleep\n",
    "\n",
    "lmunit_scores = {}\n",
    "for id, dial in batch_dials.items():\n",
    "    lmunit_scores[id] = []\n",
    "    for i, turn in enumerate(dial):\n",
    "        history = turn[\"conversation_history\"] + \"\\nCustomer: \" + turn[\"user\"]\n",
    "        db = json.dumps(turn[\"db\"])\n",
    "        # get conversation consistency score\n",
    "        conv_payload = {\n",
    "            \"query\": history,\n",
    "            \"response\": turn[\"response\"],\n",
    "            \"unit_test\": \"\"\n",
    "        }\n",
    "        conv_score = 0\n",
    "        for q in conv_qs:\n",
    "            conv_payload[\"unit_test\"] = q\n",
    "            conv = requests.post(url, json=conv_payload, headers=headers)\n",
    "            conv_score += float(json.loads(conv.text)[\"score\"])\n",
    "            sleep(3)\n",
    "        conv_score = round(conv_score/3, 2)\n",
    "        # get backend knowledge consistency score\n",
    "        backend_payload = {\n",
    "            \"query\": history + \"\\nDatabase result: \" + db,\n",
    "            \"response\": turn[\"response\"],\n",
    "            \"unit_test\": \"\"\n",
    "        }\n",
    "        backend_score = 0\n",
    "        for q in backend_qs:\n",
    "            backend_payload[\"unit_test\"] = q\n",
    "            backend = requests.post(url, json=backend_payload, headers=headers)\n",
    "            backend_score += float(json.loads(conv.text)[\"score\"])\n",
    "            sleep(3)\n",
    "        backend_score = round(backend_score/3, 2)\n",
    "        # get policy compliance score\n",
    "        policy_payload = {\n",
    "            \"query\": history + \"\\nDatabase result: \" + db,\n",
    "            \"response\": turn[\"response\"],\n",
    "            \"unit_test\": \"\"\n",
    "        }\n",
    "        policy_score = 0\n",
    "        for q in policy_qs:\n",
    "            policy_payload[\"unit_test\"] = q\n",
    "            policy = requests.post(url, json=policy_payload, headers=headers)\n",
    "            policy_score += float(json.loads(conv.text)[\"score\"])\n",
    "            sleep(3)\n",
    "        policy_score = round(policy_score/3, 2)\n",
    "        # compile scores\n",
    "        lmunit_scores[id].append({\n",
    "            \"conv_consistency\": conv_score,\n",
    "            \"backend_consistency\": backend_score,\n",
    "            \"policy_completeness\": policy_score\n",
    "        })\n",
    "    sleep(10)\n",
    "    \n",
    "print(json.dumps(lmunit_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get TD-Eval Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"pmul1537\": [{\"conv_consistency\": 5, \"backend_consistency\": 5, \"policy_completeness\": 5}, {\"conv_consistency\": 4, \"backend_consistency\": 5, \"policy_completeness\": 5}, {\"conv_consistency\": 5, \"backend_consistency\": 5, \"policy_completeness\": 5}, {\"conv_consistency\": 5, \"backend_consistency\": 5, \"policy_completeness\": 5}, {\"conv_consistency\": 5, \"backend_consistency\": 4, \"policy_completeness\": 5}, {\"conv_consistency\": 4, \"backend_consistency\": 3, \"policy_completeness\": 5}, {\"conv_consistency\": 4, \"backend_consistency\": 3, \"policy_completeness\": 5}, {\"conv_consistency\": 4, \"backend_consistency\": 4, \"policy_completeness\": 4}, {\"conv_consistency\": 2, \"backend_consistency\": 2, \"policy_completeness\": 3}, {\"conv_consistency\": 2, \"backend_consistency\": 3, \"policy_completeness\": 2}], \"40\": [{\"conv_consistency\": 5, \"backend_consistency\": 3, \"policy_completeness\": 5}, {\"conv_consistency\": 5, \"backend_consistency\": 5, \"policy_completeness\": 5}, {\"conv_consistency\": 5, \"backend_consistency\": 2, \"policy_completeness\": 4}, {\"conv_consistency\": 5, \"backend_consistency\": 3, \"policy_completeness\": 2}, {\"conv_consistency\": 4, \"backend_consistency\": 5, \"policy_completeness\": 5}, {\"conv_consistency\": 5, \"backend_consistency\": 5, \"policy_completeness\": 5}, {\"conv_consistency\": 5, \"backend_consistency\": 3, \"policy_completeness\": 5}], \"6\": [{\"conv_consistency\": 5, \"backend_consistency\": 3, \"policy_completeness\": 5}, {\"conv_consistency\": 4, \"backend_consistency\": 3, \"policy_completeness\": 5}, {\"conv_consistency\": 5, \"backend_consistency\": 5, \"policy_completeness\": 5}, {\"conv_consistency\": 4, \"backend_consistency\": 4, \"policy_completeness\": 2}, {\"conv_consistency\": 2, \"backend_consistency\": 2, \"policy_completeness\": 1}]}\n"
     ]
    }
   ],
   "source": [
    "from calculate_annotator_agreement import extract_score\n",
    "\n",
    "batch_dial_scores = {}\n",
    "for id, dial in batch_dials.items():\n",
    "    batch_dial_scores[id] = []\n",
    "    for i, turn in enumerate(dial):\n",
    "        scores = turn[\"scores\"]\n",
    "        batch_dial_scores[id].append({\n",
    "            \"conv_consistency\": extract_score(scores[\"conv_consistency\"][\"score\"]),\n",
    "            \"backend_consistency\": extract_score(scores[\"backend_consistency\"][\"score\"]),\n",
    "            \"policy_completeness\": extract_score(scores[\"policy_completeness\"][\"score\"])\n",
    "        })\n",
    "print(json.dumps(batch_dial_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get Human Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"pmul1537\": [{\"conv_consistency\": 4.5, \"backend_consistency\": 5.0, \"policy_completeness\": 5.0}, {\"conv_consistency\": 5.0, \"backend_consistency\": 5.0, \"policy_completeness\": 5.0}, {\"conv_consistency\": 4.5, \"backend_consistency\": 5.0, \"policy_completeness\": 4.5}, {\"conv_consistency\": 5.0, \"backend_consistency\": 5.0, \"policy_completeness\": 4.5}, {\"conv_consistency\": 4.0, \"backend_consistency\": 5.0, \"policy_completeness\": 3.5}, {\"conv_consistency\": 5.0, \"backend_consistency\": 5.0, \"policy_completeness\": 5.0}, {\"conv_consistency\": 4.0, \"backend_consistency\": 5.0, \"policy_completeness\": 4.0}, {\"conv_consistency\": 4.0, \"backend_consistency\": 4.5, \"policy_completeness\": 4.5}, {\"conv_consistency\": 3.5, \"backend_consistency\": 4.5, \"policy_completeness\": 4.0}, {\"conv_consistency\": 3.5, \"backend_consistency\": 5.0, \"policy_completeness\": 3.5}], \"40\": [{\"conv_consistency\": 5.0, \"backend_consistency\": 5.0, \"policy_completeness\": 5.0}, {\"conv_consistency\": 5.0, \"backend_consistency\": 5.0, \"policy_completeness\": 5.0}, {\"conv_consistency\": 5.0, \"backend_consistency\": 5.0, \"policy_completeness\": 5.0}, {\"conv_consistency\": 4.5, \"backend_consistency\": 5.0, \"policy_completeness\": 4.5}, {\"conv_consistency\": 4.5, \"backend_consistency\": 5.0, \"policy_completeness\": 5.0}, {\"conv_consistency\": 5.0, \"backend_consistency\": 5.0, \"policy_completeness\": 5.0}, {\"conv_consistency\": 5.0, \"backend_consistency\": 4.5, \"policy_completeness\": 5.0}], \"6\": [{\"conv_consistency\": 5.0, \"backend_consistency\": 5.0, \"policy_completeness\": 5.0}, {\"conv_consistency\": 5.0, \"backend_consistency\": 5.0, \"policy_completeness\": 5.0}, {\"conv_consistency\": 5.0, \"backend_consistency\": 5.0, \"policy_completeness\": 5.0}, {\"conv_consistency\": 4.0, \"backend_consistency\": 5.0, \"policy_completeness\": 5.0}, {\"conv_consistency\": 3.0, \"backend_consistency\": 4.0, \"policy_completeness\": 5.0}]}\n"
     ]
    }
   ],
   "source": [
    "with open(human_eval_scores_json, 'r') as f:\n",
    "    human_eval_scores = json.load(f)\n",
    "human_scores = human_eval_scores[\"turn_level_scores\"]\n",
    "\n",
    "human_dial_scores = {}\n",
    "for id, dial in batch_dials.items():\n",
    "    human_dial_scores[id] = []\n",
    "    dial_indices = indices[id]\n",
    "    for ind in range(dial_indices[0], dial_indices[1]):\n",
    "        human_dial_scores[id].append({\n",
    "            \"conv_consistency\": human_scores[\"avg_human_conv_consistency_turn_scores\"][ind],\n",
    "            \"backend_consistency\": human_scores[\"avg_human_backend_consistency_turn_scores\"][ind],\n",
    "            \"policy_completeness\": human_scores[\"avg_human_policy_completeness_turn_scores\"][ind],\n",
    "        })\n",
    "print(json.dumps(human_dial_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"human_tdeval\": {\"conv_consistency\": {\"pearson\": {\"score\": 0.79, \"p_value\": 0.0}, \"spearman\": {\"score\": 0.6, \"p_value\": 0.0}}, \"backend_consistency\": {\"pearson\": {\"score\": 0.45, \"p_value\": 0.03}, \"spearman\": {\"score\": 0.43, \"p_value\": 0.04}}, \"policy_completeness\": {\"pearson\": {\"score\": 0.22, \"p_value\": 0.31}, \"spearman\": {\"score\": 0.26, \"p_value\": 0.24}}}, \"human_lmunit\": {\"conv_consistency\": {\"pearson\": {\"score\": 0.15, \"p_value\": 0.5}, \"spearman\": {\"score\": 0.23, \"p_value\": 0.29}}, \"backend_consistency\": {\"pearson\": {\"score\": -0.11, \"p_value\": 0.63}, \"spearman\": {\"score\": -0.11, \"p_value\": 0.63}}, \"policy_completeness\": {\"pearson\": {\"score\": 0.51, \"p_value\": 0.02}, \"spearman\": {\"score\": 0.55, \"p_value\": 0.01}}}}\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "compiled_human_scores = {\n",
    "    \"conv_consistency\": [],\n",
    "    \"backend_consistency\": [],\n",
    "    \"policy_completeness\": []\n",
    "}\n",
    "for id, scores in human_dial_scores.items():\n",
    "    for turn_score in scores:\n",
    "        compiled_human_scores[\"conv_consistency\"].append(turn_score[\"conv_consistency\"])\n",
    "        compiled_human_scores[\"backend_consistency\"].append(turn_score[\"backend_consistency\"])\n",
    "        compiled_human_scores[\"policy_completeness\"].append(turn_score[\"policy_completeness\"])\n",
    "        \n",
    "compiled_tdeval_scores = {\n",
    "    \"conv_consistency\": [],\n",
    "    \"backend_consistency\": [],\n",
    "    \"policy_completeness\": []\n",
    "}\n",
    "for id, scores in batch_dial_scores.items():\n",
    "    for turn_score in scores:\n",
    "        compiled_tdeval_scores[\"conv_consistency\"].append(turn_score[\"conv_consistency\"])\n",
    "        compiled_tdeval_scores[\"backend_consistency\"].append(turn_score[\"backend_consistency\"])\n",
    "        compiled_tdeval_scores[\"policy_completeness\"].append(turn_score[\"policy_completeness\"])\n",
    "\n",
    "compiled_lmunit_scores = {\n",
    "    \"conv_consistency\": [],\n",
    "    \"backend_consistency\": [],\n",
    "    \"policy_completeness\": []\n",
    "}\n",
    "for id, scores in lmunit_scores.items():\n",
    "    for turn_score in scores:\n",
    "        compiled_lmunit_scores[\"conv_consistency\"].append(turn_score[\"conv_consistency\"])\n",
    "        compiled_lmunit_scores[\"backend_consistency\"].append(turn_score[\"backend_consistency\"])\n",
    "        compiled_lmunit_scores[\"policy_completeness\"].append(turn_score[\"policy_completeness\"])  \n",
    "        \n",
    "compiled_corrs = {}\n",
    "\n",
    "conv_pear_stat, conv_pear_pval = stats.pearsonr(x=compiled_human_scores[\"conv_consistency\"], y=compiled_tdeval_scores[\"conv_consistency\"])\n",
    "backend_pear_stat, backend_pear_pval = stats.pearsonr(x=compiled_human_scores[\"backend_consistency\"], y=compiled_tdeval_scores[\"backend_consistency\"])\n",
    "policy_pear_stat, policy_pear_pval = stats.pearsonr(x=compiled_human_scores[\"policy_completeness\"], y=compiled_tdeval_scores[\"policy_completeness\"])\n",
    "conv_spear_stat, conv_spear_pval = stats.spearmanr(a=compiled_human_scores[\"conv_consistency\"], b=compiled_tdeval_scores[\"conv_consistency\"])\n",
    "backend_spear_stat, backend_spear_pval = stats.spearmanr(a=compiled_human_scores[\"backend_consistency\"], b=compiled_tdeval_scores[\"backend_consistency\"])\n",
    "policy_spear_stat, policy_spear_pval = stats.spearmanr(a=compiled_human_scores[\"policy_completeness\"], b=compiled_tdeval_scores[\"policy_completeness\"])\n",
    "compiled_corrs[\"human_tdeval\"] = {\n",
    "    \"conv_consistency\": {\n",
    "        \"pearson\": {\"score\": round(conv_pear_stat,2), \"p_value\": round(conv_pear_pval, 2)},\n",
    "        \"spearman\": {\"score\": round(conv_spear_stat,2), \"p_value\": round(conv_spear_pval,2)}\n",
    "    },\n",
    "    \"backend_consistency\": {\n",
    "        \"pearson\": {\"score\": round(backend_pear_stat,2), \"p_value\": round(backend_pear_pval,2)},\n",
    "        \"spearman\": {\"score\": round(backend_spear_stat,2), \"p_value\": round(backend_spear_pval,2)}\n",
    "    },\n",
    "    \"policy_completeness\": {\n",
    "        \"pearson\": {\"score\": round(policy_pear_stat,2), \"p_value\": round(policy_pear_pval,2)},\n",
    "        \"spearman\": {\"score\": round(policy_spear_stat,2), \"p_value\": round(policy_spear_pval,2)}\n",
    "    }\n",
    "}\n",
    "\n",
    "conv_pear_stat, conv_pear_pval = stats.pearsonr(x=compiled_human_scores[\"conv_consistency\"], y=compiled_lmunit_scores[\"conv_consistency\"])\n",
    "backend_pear_stat, backend_pear_pval = stats.pearsonr(x=compiled_human_scores[\"backend_consistency\"], y=compiled_lmunit_scores[\"backend_consistency\"])\n",
    "policy_pear_stat, policy_pear_pval = stats.pearsonr(x=compiled_human_scores[\"policy_completeness\"], y=compiled_lmunit_scores[\"policy_completeness\"])\n",
    "conv_spear_stat, conv_spear_pval = stats.spearmanr(a=compiled_human_scores[\"conv_consistency\"], b=compiled_lmunit_scores[\"conv_consistency\"])\n",
    "backend_spear_stat, backend_spear_pval = stats.spearmanr(a=compiled_human_scores[\"backend_consistency\"], b=compiled_lmunit_scores[\"backend_consistency\"])\n",
    "policy_spear_stat, policy_spear_pval = stats.spearmanr(a=compiled_human_scores[\"policy_completeness\"], b=compiled_lmunit_scores[\"policy_completeness\"])\n",
    "compiled_corrs[\"human_lmunit\"] = {\n",
    "    \"conv_consistency\": {\n",
    "        \"pearson\": {\"score\": round(conv_pear_stat,2), \"p_value\": round(conv_pear_pval, 2)},\n",
    "        \"spearman\": {\"score\": round(conv_spear_stat,2), \"p_value\": round(conv_spear_pval,2)}\n",
    "    },\n",
    "    \"backend_consistency\": {\n",
    "        \"pearson\": {\"score\": round(backend_pear_stat,2), \"p_value\": round(backend_pear_pval,2)},\n",
    "        \"spearman\": {\"score\": round(backend_spear_stat,2), \"p_value\": round(backend_spear_pval,2)}\n",
    "    },\n",
    "    \"policy_completeness\": {\n",
    "        \"pearson\": {\"score\": round(policy_pear_stat,2), \"p_value\": round(policy_pear_pval,2)},\n",
    "        \"spearman\": {\"score\": round(policy_spear_stat,2), \"p_value\": round(policy_spear_pval,2)}\n",
    "    }\n",
    "}\n",
    "\n",
    "print(json.dumps(compiled_corrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
