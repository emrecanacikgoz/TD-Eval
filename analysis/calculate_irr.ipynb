{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92a9a1e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import pprint\n",
    "import seaborn as sns\n",
    "from krippendorff import alpha\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa, aggregate_raters, cohens_kappa, to_table\n",
    "from irrCAC.raw import CAC\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3ffe0d",
   "metadata": {},
   "source": [
    "### Set up variables and load conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629bb97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_eval_csv = \"qualtrics/results/main_human_eval/pilot-eval-irr.csv\"\n",
    "autotod_mwoz_eval_json = \"results/judge_results_mwoz_autotod/20250403_025805/mwoz-autotod-gpt-4o_j.json\"\n",
    "tau_air_eval_json = \"results/judge-results-tau/20250131_152503-tau-4o-airline/tau-gpt-4o_j.json\"\n",
    "tau_retail_eval_json = \"results/judge-results-tau/20250131_152422-tau-4o-retail/tau-gpt-4o_j.json\"\n",
    "dial_batch_json = \"datasets/main_human_eval/irr_eval_batch.json\"\n",
    "num_annotators = 10\n",
    "\n",
    "# load dialogues\n",
    "with open(autotod_mwoz_eval_json, 'r') as f:\n",
    "\t\tautotod_mwoz_eval = json.load(f)\n",
    "autotod_mwoz_dials = autotod_mwoz_eval.get('dialogues', [])\n",
    "with open(tau_air_eval_json, 'r') as f:\n",
    "\t\ttau_air_eval = json.load(f)\n",
    "tau_air_dials = tau_air_eval['dialogues']\n",
    "with open(tau_retail_eval_json, 'r') as f:\n",
    "\t\ttau_retail_eval = json.load(f)\n",
    "tau_retail_dials = tau_retail_eval['dialogues']\n",
    "# load batches\n",
    "batch_list = None\n",
    "with open(dial_batch_json, 'r') as f:\n",
    "\t\tbatch_list = json.load(f)\n",
    "if batch_list is None or len(batch_list) == 0:\n",
    "\t\tprint('No batches found at this path:',  dial_batch_json)\n",
    "\t\texit()\n",
    "batch_order = batch_list[\"order\"]\n",
    "print(\"batch order:\")\n",
    "pprint.pprint(batch_order, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69bcbb2",
   "metadata": {},
   "source": [
    "### Order dialogues into batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df98a20",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def get_batch_dialogues(\n",
    "\tmwoz_dialogues: dict, \n",
    "\ttau_air_dialogues: dict, \n",
    "\ttau_retail_dialogues: dict, \n",
    "\tbatch_list: dict\n",
    ") -> dict:\n",
    "\t#load dialogues\n",
    "\tbatch_dials = []\n",
    "\tmwoz_batch_ids = batch_list[\"autotod_mwoz\"]\n",
    "\ttau_air_batch_ids = batch_list[\"tau\"][\"airline\"]\n",
    "\ttau_retail_batch_ids = batch_list[\"tau\"][\"retail\"]\n",
    "\t# load batch \n",
    "\tfor batch_id in mwoz_batch_ids:\n",
    "\t\tfor id, dial in mwoz_dialogues.items():\n",
    "\t\t\tif id.split(\".json\")[0].lower() == batch_id:\n",
    "\t\t\t\tbatch_dials.append(dial)\n",
    "\t\t\t\tbreak\n",
    "\tfor batch_id in tau_air_batch_ids:\n",
    "\t\tfor id, dial in tau_air_dialogues.items():\n",
    "\t\t\tif id == batch_id:\n",
    "\t\t\t\tbatch_dials.append(dial)\n",
    "\t\t\t\tbreak\n",
    "\tfor batch_id in tau_retail_batch_ids:\n",
    "\t\tfor id, dial in tau_retail_dialogues.items():\n",
    "\t\t\tif id == batch_id:\n",
    "\t\t\t\tbatch_dials.append(dial)\n",
    "\t\t\t\tbreak\n",
    "\ttot_batch_len = len(mwoz_batch_ids) + len(tau_air_batch_ids) + len(tau_retail_batch_ids)\n",
    "\tif len(batch_dials) != tot_batch_len:\n",
    "\t\tprint(\"filtered dials size does not match batches:\", len(batch_dials), tot_batch_len)\n",
    "\t\texit()\n",
    "\treturn batch_dials\n",
    "\n",
    "batch_dialogues = get_batch_dialogues(\n",
    "\tautotod_mwoz_dials, \n",
    "\ttau_air_dials, \n",
    "\ttau_retail_dials, \n",
    "\tbatch_list\n",
    ")\n",
    "print(\"batch dialogues:\") \n",
    "pprint.pprint(batch_dialogues, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e3159f",
   "metadata": {},
   "source": [
    "### Process Qualtrics Human Eval Scores From CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af4599f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def process_extracted_human_csv_data(\n",
    "\tinput_file: str, \n",
    "\tbatch_dialogues: list, \n",
    "\tbatch_order: dict\n",
    ") -> dict:\n",
    "\t\"\"\"Read CSV data and convert to appropriate format\"\"\"\n",
    "\teval_csv = pd.read_csv(input_file, on_bad_lines='warn') \n",
    "\tstart_col = 'QID100_1'\n",
    "\tend_col = 'QID130_3'\n",
    "\tsearch_str = '2025'\n",
    "\tturn_result = {}\n",
    "\tdial_result = {}\n",
    "\tfirst_eval_row = eval_csv.StartDate.str.contains(search_str).idxmax()\n",
    "\thuman_scores = eval_csv.loc[first_eval_row:, start_col:end_col].to_numpy()\n",
    "\tmapping = {\n",
    "\t\t\"Very Good\": 5.0, \n",
    "\t\t\"Good\": 4.0, \n",
    "\t\t\"Fair\": 3.0, \n",
    "\t\t\"Bad\": 2.0, \n",
    "\t\t\"Very Bad\": 1.0\n",
    "\t}\n",
    "\t# mapping = {\n",
    "\t# \t\"Very Good\": 3.0, \n",
    "\t# \t\"Good\": 3.0, \n",
    "\t# \t\"Fair\": 2.0, \n",
    "\t# \t\"Bad\": 1.0, \n",
    "\t# \t\"Very Bad\": 1.0\n",
    "\t# }\n",
    "\tvectorized_map = np.vectorize(lambda x: mapping[x])\n",
    "\tint_scores = vectorized_map(human_scores)\n",
    "\t# extract scores into results dialogue map\n",
    "\tscores_idx = 0\n",
    "\tfor i, dial in enumerate(batch_dialogues):\n",
    "\t\tdial_id = batch_order[i][\"id\"]\n",
    "\t\tturn_result[dial_id] = []\n",
    "\t\t# add turn scores\n",
    "\t\tfor _ in dial:\n",
    "\t\t\tturn_result[dial_id].append({\n",
    "\t\t\t\t'conv_consistency': int_scores[:,scores_idx],\n",
    "\t\t\t\t'backend_consistency': int_scores[:,scores_idx+1],\n",
    "\t\t\t\t'policy_completeness': int_scores[:, scores_idx+2]\n",
    "\t\t\t})\n",
    "\t\t\tscores_idx += 3\n",
    "\t\t# add dial scores\n",
    "\t\tdial_result[dial_id] = {\n",
    "\t\t\t\t'conv_consistency': int_scores[:,scores_idx],\n",
    "\t\t\t\t'backend_consistency': int_scores[:,scores_idx+1],\n",
    "\t\t\t\t'policy_completeness': int_scores[:, scores_idx+2]\n",
    "\t\t}\n",
    "\t\tscores_idx += 3\n",
    "\treturn turn_result, dial_result\n",
    "\n",
    "turn_eval_data, dial_eval_data = process_extracted_human_csv_data(\n",
    "\thuman_eval_csv, \n",
    "\tbatch_dialogues, \n",
    "\tbatch_order\n",
    ")\n",
    "print(\"turn eval data:\") \n",
    "pprint.pprint(turn_eval_data, compact=True)\n",
    "print(\"dial eval data\")\n",
    "pprint.pprint(dial_eval_data, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a450db65",
   "metadata": {},
   "source": [
    "### Compile Human Eval Scores into dictionary for IRR Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57fe1b1",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def compile_eval_scores(\n",
    "\tturn_eval_data: dict, \n",
    "\tdial_eval_data: dict, \n",
    "\tnum_annotators: int\n",
    ") -> dict:\n",
    "\t\"\"\"Calculate agreement metrics between annotators and between annotators and LLM\"\"\"\n",
    "\tannotator_turn_data = {\n",
    "\t\t'conv_consistency': np.empty((num_annotators, 0), dtype=float),\n",
    "\t\t'backend_consistency': np.empty((num_annotators, 0), dtype=float),\n",
    "\t\t'policy_completeness': np.empty((num_annotators, 0), dtype=float),\n",
    "\t\t'overall': np.empty((num_annotators, 0), dtype=float)\n",
    "\t}\n",
    "\tannotator_dial_data = {\n",
    "\t\t'conv_consistency': np.empty((num_annotators, 0), dtype=float),\n",
    "\t\t'backend_consistency': np.empty((num_annotators, 0), dtype=float),\n",
    "\t\t'policy_completeness': np.empty((num_annotators, 0), dtype=float),\n",
    "\t\t'overall': np.empty((num_annotators, 0), dtype=float)\n",
    "\n",
    "\t}\n",
    "\n",
    "\t# compile turn scores\n",
    "\tfor annotator_turns_scores in turn_eval_data.values():\n",
    "\t\tfor turn_score in annotator_turns_scores:\n",
    "\t\t\t# skip turn score if any negative/invalid scores exist\n",
    "\t\t\tall_scores = np.concat((\n",
    "\t\t\t\tturn_score['conv_consistency'], \n",
    "\t\t\t\tturn_score['backend_consistency'], \n",
    "\t\t\t\tturn_score['policy_completeness']\n",
    "\t\t\t))\n",
    "\t\t\tif np.any(all_scores <= 0):\n",
    "\t\t\t\tcontinue\n",
    "\t\t\t# compile reliability matrices for inter-annotator agreement\n",
    "\t\t\tfor metric, score in turn_score.items():\n",
    "\t\t\t\tannotator_turn_data[metric] = np.hstack((\n",
    "\t\t\t\t\tannotator_turn_data[metric], \n",
    "\t\t\t\t\tnp.reshape(score, shape=(num_annotators,1))\n",
    "\t\t\t\t))\n",
    "\t\t\t\tannotator_turn_data[\"overall\"] = np.hstack((\n",
    "\t\t\t\t\tannotator_turn_data[\"overall\"],\n",
    "\t\t\t\t\tnp.reshape(score, shape=(num_annotators,1))\n",
    "\t\t\t\t))\n",
    "\t\t\t\n",
    "\t# compile dialogue scores\n",
    "\tfor annotator_dials_score in dial_eval_data.values():\n",
    "\t\t# skip turn score if any negative/invalid scores exist\n",
    "\t\tall_scores = np.concat((\n",
    "\t\t\tannotator_dials_score['conv_consistency'], \n",
    "\t\t\tannotator_dials_score['backend_consistency'], \n",
    "\t\t\tannotator_dials_score['policy_completeness']\n",
    "\t\t))\n",
    "\t\tif np.any(all_scores <= 0):\n",
    "\t\t\tcontinue\n",
    "\t\tfor metric, score in annotator_dials_score.items():\n",
    "\t\t\t# compile reliability matrices for inter-annotator agreement\n",
    "\t\t\tannotator_dial_data[metric] = np.hstack((\n",
    "\t\t\t\tannotator_dial_data[metric], \n",
    "\t\t\t\tnp.reshape(score, shape=(num_annotators,1))\n",
    "\t\t\t))\n",
    "\t\t\tprint(annotator_dial_data[metric].shape)\n",
    "\t\t\tannotator_dial_data[\"overall\"] = np.hstack((\n",
    "\t\t\t\tannotator_dial_data[\"overall\"],\n",
    "\t\t\t\tnp.reshape(score, shape=(num_annotators,1))\n",
    "\t\t\t))\n",
    "\treturn { \"turn\": annotator_dial_data, \"dial\": annotator_dial_data }\n",
    "\n",
    "eval_scores = compile_eval_scores(\n",
    "\tturn_eval_data, \n",
    "\tdial_eval_data, \n",
    "\tnum_annotators\n",
    ")\n",
    "print(\"eval scores\")\n",
    "pprint.pprint(eval_scores, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e49f1a4",
   "metadata": {},
   "source": [
    "### Calculate Data Distribution and Disagreement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c7784",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disagreement_matrix(scores: dict):\n",
    "\tplt.figure(figsize=(15, 10))\n",
    "\tplt.suptitle('Agreement Heatmaps of Conversation Scores')\n",
    "\tdims = ['conv_consistency', 'backend_consistency', 'policy_completeness']\n",
    "\n",
    "\t# Define the colormap\n",
    "\tcolors = [(0, 1, 0), (1, 1, 1), (1, 0, 0)]  # Green -> White -> Red\n",
    "\tn_bins = 100  # Discretize the interpolation into bins\n",
    "\tcmap_name = 'green_red'\n",
    "\tcm = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)\n",
    "\tfor i, metric in enumerate(dims, 1):\n",
    "\t\tannotator_matrix = np.concat(\n",
    "\t\t\t(scores[\"turn\"][metric], scores[\"dial\"][metric]),\n",
    "\t\t\taxis=1\n",
    "\t\t).astype(dtype=np.int64)\n",
    "\t\t# Compute the columnâ€wise majority vote\n",
    "\t\tmaj = np.apply_along_axis(\n",
    "\t\t\tlambda col: np.bincount(col).argmax(), axis=0, \n",
    "\t\t\tarr=annotator_matrix\n",
    "\t\t)\n",
    "\t\t# Build a disagreement mask (0 = agree, 1 = disagree)\n",
    "\t\tdisagree = (annotator_matrix != maj).astype(int)\n",
    "\t\tplt.subplot(2, 2, i)\n",
    "\t\tax = sns.heatmap(\n",
    "\t\t\tdisagree,\n",
    "\t\t\tcmap=cm,\n",
    "\t\t\tcbar_kws={\"label\": \"Disagreement (1=Yes)\"},\n",
    "\t\t\tlinewidths=0.5,\n",
    "\t\t\tlinecolor=\"lightgray\",\n",
    "\t\t\tsquare=False\n",
    "\t\t)\n",
    "\t\tax.set_xlabel(\"Score Index\")\n",
    "\t\tax.set_ylabel(\"Annotator Index\")\n",
    "\t\tax.set_yticklabels(\n",
    "\t\t\t[f\"Ann {i}\" for i in range(annotator_matrix.shape[0])], \n",
    "\t\t\trotation=0\n",
    "\t\t)\n",
    "\t\tmetric_title = metric.replace('_', ' ').title()\n",
    "\t\tax.set_title(f\"{metric_title}: Annotator vs. Majority Disagreement Heatmap\")\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\n",
    "generate_disagreement_matrix(eval_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e461419f",
   "metadata": {},
   "source": [
    "### Prune Score Matrix of Bad Annotators "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a60a3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_histograms(scores: dict, drop_annotators: list):#, output_dir):\n",
    "\t# Histogram for 1-10 scale metrics\n",
    "\tplt.figure(figsize=(15, 10))\n",
    "\tplt.suptitle('Histograms of Dialogue Turn Scores (1-5 Scale)')\n",
    "    \n",
    "\tdims = ['conv_consistency', 'backend_consistency', 'policy_completeness']\n",
    "\tfor i, metric in enumerate(dims, 1):\n",
    "\t\t# compile score matrix and prune bad annotators\n",
    "\t\tannotator_matrix = np.concat(\n",
    "\t\t\t(scores[\"turn\"][metric], scores[\"dial\"][metric]),\n",
    "\t\t\taxis=1\n",
    "\t\t).astype(dtype=np.int64)\n",
    "\t\tpruned_matrix = np.delete(annotator_matrix, drop_annotators, axis=0)\n",
    "\t\t# flatten score matrix and create histogram \n",
    "\t\tall_scores = pruned_matrix.flatten()\n",
    "\t\tplt.subplot(2, 2, i)\n",
    "\t\tplt.hist(all_scores, bins=np.arange(0.5, 6.5, 1), edgecolor='black')\n",
    "\t\tplt.title(metric.replace('_', ' ').title())\n",
    "\t\tplt.xticks(np.arange(1, 6, 1))\n",
    "\t\tplt.xlabel('Score')\n",
    "\t\tplt.ylabel('Frequency')\n",
    "\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\t# plt.savefig(os.path.join(output_dir, 'score_histograms_1_5.png')) \n",
    "\n",
    "drop_annotators = [] #[3, 5, 8] # [1, 2, 3, 5, 8]\n",
    "pruned_new_annotators = num_annotators - len(drop_annotators)\n",
    "generate_histograms(eval_scores, drop_annotators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe5fa1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculate_percent_agreement(scores: dict, drop_annotators: list) -> dict:\n",
    "\tdims = [\n",
    "\t\t'conv_consistency', \n",
    "\t\t'backend_consistency', \n",
    "\t\t'policy_completeness', \n",
    "\t\t'overall'\n",
    "\t]\n",
    "\tpercent_agreement = {}\n",
    "\tfor metric in dims:\n",
    "\t\tprint(scores[\"turn\"][metric].shape, scores[\"dial\"][metric].shape)\n",
    "\t\tannotator_matrix = np.concat(\n",
    "\t\t\t(scores[\"turn\"][metric], scores[\"dial\"][metric]),\n",
    "\t\t\taxis=1\n",
    "\t\t).astype(dtype=np.int64)\n",
    "\t\tpruned_matrix = np.delete(annotator_matrix, drop_annotators, axis=0)\n",
    "\t\tagree_array = np.all(pruned_matrix==pruned_matrix[0:1, :], axis=0)\n",
    "\t\tpercent_agreement[metric] = float(agree_array.mean())\n",
    "\treturn percent_agreement\n",
    "\n",
    "p_agree = calculate_percent_agreement(eval_scores, drop_annotators)\n",
    "pprint.pprint(p_agree, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096c8e81",
   "metadata": {},
   "source": [
    "### Calculate IRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daa7dbf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def calculate_krippendorff_alpha(data: np.ndarray) -> float:\n",
    "\t\"\"\"Calculate Krippendorff's alpha for ordinal data\"\"\"\n",
    "\ttry:\n",
    "\t\treturn alpha(\n",
    "\t\t\treliability_data=data.astype(np.int64), \n",
    "\t\t\tvalue_domain=[1,2,3,4,5], \n",
    "\t\t\tlevel_of_measurement='interval'\n",
    "\t\t)\n",
    "\texcept Exception as e:\n",
    "\t\t\tprint(f\"Krippendorff calculation error: {e}\")\n",
    "\t\t\treturn None\n",
    "\n",
    "def calculate_kappa(data: np.ndarray, n_cat: int) -> dict:\n",
    "\t\"\"\"Calculate kappa statistic for more than 2 raters (fleiss and randolph)\"\"\"\n",
    "\ttry:\n",
    "\t\tdata_table, _ = aggregate_raters(data=data, n_cat=n_cat)\n",
    "\t\t# randolph method gives best performance\n",
    "\t\tfleiss = fleiss_kappa(table=data_table, method='fleiss') \n",
    "\t\trandolph = fleiss_kappa(table=data_table, method='randolph') \n",
    "\t\treturn {\"fleiss\": fleiss, \"randolph\": randolph}\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"Fleiss calculation error: {e}\")\n",
    "\t\treturn None\n",
    "\n",
    "def calculate_cohen_kappa(data: np.ndarray, n_cat: int) -> float:\n",
    "\t\"\"\"Calculate Cohen's kappa for 2 raters\"\"\"\n",
    "\ttry:\n",
    "\t\tdata_table, _ = to_table(data=data, bins=n_cat)\n",
    "\t\treturn cohens_kappa(table=data_table, wt='linear')['kappa']\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f\"Cohen calculation error: {e}\")\n",
    "\t\treturn None\n",
    "\t\n",
    "def calculate_gwet_ac(data: np.ndarray) -> float:\n",
    "\t\"\"\"Calculate Gwet's AC1 for data with skew in distribution\"\"\"\n",
    "\tcac_raters = CAC(pd.DataFrame(data))\n",
    "\tgwet = cac_raters.gwet()\n",
    "\t# pprint.pprint(gwet, compact=True)\n",
    "\treturn float(gwet['est']['coefficient_value'])\n",
    "\n",
    "def calculate_irr(\n",
    "\tnum_annotators: int, \n",
    "\tscores: dict, \n",
    "\tdrop_annotators: list,\n",
    "\tdebug: bool):\n",
    "\t\"\"\"Calculate agreement metrics between annotators and between annotators and LLM\"\"\"\n",
    "\tmetrics = {}    \n",
    "\t# calculate irr scores\n",
    "\tdims = [\n",
    "\t\t'conv_consistency', \n",
    "\t\t'backend_consistency', \n",
    "\t\t'policy_completeness', \n",
    "\t\t'overall'\n",
    "\t]\n",
    "\tfor metric in dims:\n",
    "\t\t# compile matrix and prune annotators\n",
    "\t\tannotator_matrix = np.concat(\n",
    "\t\t\t(scores[\"turn\"][metric], scores[\"dial\"][metric]),\n",
    "\t\t\taxis=1\n",
    "\t\t).astype(dtype=np.int64)\n",
    "\t\tpruned_matrix = np.delete(annotator_matrix, drop_annotators, axis=0)\n",
    "\t\t# calculate irr scores\n",
    "\t\tk_alpha = calculate_krippendorff_alpha(pruned_matrix)\n",
    "\t\tprocessed_data = pruned_matrix.T.astype(np.int64) - 1\n",
    "\t\tkappa = calculate_kappa(processed_data, 5)\n",
    "\t\tf_kappa = kappa['fleiss']\n",
    "\t\tr_kappa = kappa['randolph']\n",
    "\t\tgwet_ac = calculate_gwet_ac(pruned_matrix)\n",
    "\t\tmetrics[metric] = {\n",
    "\t\t\t'k_alpha': float(k_alpha) if k_alpha is not None else 0.0,\n",
    "\t\t\t'f_kappa': float(f_kappa) if f_kappa is not None else 0.0,\n",
    "\t\t\t'r_kappa': float(r_kappa) if r_kappa is not None else 0.0,\n",
    "\t\t\t'gwet_ac1': float(gwet_ac) if gwet_ac is not None else 0.0\n",
    "\t\t}\n",
    "\t\t# debug info\n",
    "\t\tif debug:\n",
    "\t\t\tall_human_scores = []\n",
    "\t\t\tfor annotator_idx in range(num_annotators):\n",
    "\t\t\t\tscores_array = np.array(pruned_matrix[annotator_idx])\n",
    "\t\t\t\tvalid_scores = scores_array[~np.isnan(scores_array)]\n",
    "\t\t\t\tall_human_scores.extend(valid_scores)\n",
    "\t\t\t\tprint(f\"\\nDebug information for {metric}:\")\n",
    "\t\t\t\tprint(f\"Number of valid scores per annotator:\")\n",
    "\t\t\t\tfor i in range(num_annotators):\n",
    "\t\t\t\t\tvalid_count = np.sum(~np.isnan(pruned_matrix[i]))\n",
    "\t\t\t\t\tprint(f\"Annotator {i}: {valid_count}\")\n",
    "\t\t\t\tprint(f\"Total samples with valid scores: {len(all_human_scores)}\")\n",
    "\treturn metrics\n",
    "\n",
    "agreement_metrics = calculate_irr(\n",
    "\tpruned_new_annotators, \n",
    "\teval_scores, \n",
    "\tdrop_annotators,\n",
    "\tFalse\n",
    ")\n",
    "print(\"agreement metrics\") \n",
    "pprint.pprint(agreement_metrics, compact=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d347f8c2",
   "metadata": {},
   "source": [
    "### Calculate Pairwise (Cohen) IRR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c95de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pairwise_irr(\n",
    "\tnum_annotators: int, \n",
    "\tscores: dict, \n",
    "\tdrop_annotators: list) -> float:\n",
    "\t\"\"\"Get pairwise IRR scores for all annotators, then return average\"\"\"\n",
    "\tplt.figure(figsize=(15, 10))\n",
    "\tplt.suptitle('Agreement Heatmaps of Conversation Scores')\n",
    "\t# Define the colormap\n",
    "\tcolors = [(1, 0, 0), (1, 1, 1), (0, 1, 0)]  # Green -> White -> Red\n",
    "\tn_bins = 100  # Discretize the interpolation into bins\n",
    "\tcmap_name = 'red_green'\n",
    "\tcm = LinearSegmentedColormap.from_list(cmap_name, colors, N=n_bins)\n",
    "\t# calculate irr scores\n",
    "\tdims = ['conv_consistency', 'backend_consistency', 'policy_completeness', 'overall']\n",
    "\tfor i, metric in enumerate(dims, 1):\n",
    "\t\t# compile matrix and prune annotators\n",
    "\t\tannotator_matrix = np.concat(\n",
    "\t\t\t(scores[\"turn\"][metric], scores[\"dial\"][metric]),\n",
    "\t\t\taxis=1\n",
    "\t\t).astype(dtype=np.int64)\n",
    "\t\tpruned_matrix = np.delete(annotator_matrix, drop_annotators, axis=0)\n",
    "\t\tirr_pairwise = np.zeros(shape=(num_annotators, num_annotators))\n",
    "\t\tnum_calcs = 0\n",
    "\t\tfor a1 in range(0, num_annotators):\n",
    "\t\t\tfor a2 in range(a1+1, num_annotators):\n",
    "\t\t\t\tpairwise_data = np.array([pruned_matrix[a1], pruned_matrix[a2]]).T - 1\n",
    "\t\t\t\tirr_score = calculate_cohen_kappa(pairwise_data, 5)\n",
    "\t\t\t\tirr_pairwise[a1][a2] = irr_score if not np.isnan(irr_score) else 0\n",
    "\t\t\t\tnum_calcs += 1\n",
    "\t\tavg_irr = irr_pairwise.sum() / num_calcs\n",
    "\t\tprint(avg_irr)\n",
    "\t\tplt.subplot(2, 2, i)\n",
    "\t\tax = sns.heatmap(\n",
    "\t\t\tirr_pairwise,\n",
    "\t\t\tcmap=cm,\n",
    "\t\t\tlinewidths=0.5,\n",
    "\t\t\tlinecolor=\"lightgray\",\n",
    "\t\t\tsquare=False\n",
    "\t\t)\n",
    "\t\tax.set_xlabel(\"Annotator Index\")\n",
    "\t\tax.set_ylabel(\"Annotator Index\")\n",
    "\t\tax.set_yticklabels(\n",
    "\t\t\t[f\"Ann {i}\" for i in range(pruned_matrix.shape[0])], \n",
    "\t\t\trotation=0\n",
    "\t\t)\n",
    "\t\tmetric_title = metric.replace('_', ' ').title()\n",
    "\t\tax.set_title(f\"{metric_title}: Annotator vs. Majority Disagreement Heatmap\")\n",
    "\tplt.show()\n",
    "\treturn\n",
    "\n",
    "calculate_pairwise_irr(\t\n",
    "\tpruned_new_annotators, \n",
    "\teval_scores, \n",
    "\tdrop_annotators\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485ec177",
   "metadata": {},
   "source": [
    "### Write Results to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376c8d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save results\n",
    "output_dir = 'agreement_scores/' + os.path.basename(human_eval_csv).split('.')[0]\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "with open(os.path.join(output_dir, 'human_llm_comparison.json'), 'w') as f:\n",
    "\t\tjson.dump(agreement_metrics, f, indent=4)    \n",
    "print(f\"\\nResults saved to {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "turn-eval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
